{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aanas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aanas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\aanas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2248 entries, 0 to 2247\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   artist         2248 non-null   object\n",
      " 1   title          2248 non-null   object\n",
      " 2   mood           2248 non-null   int64 \n",
      " 3   original_mood  2248 non-null   object\n",
      " 4   lyrics         2215 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 87.9+ KB\n",
      "None\n",
      "              mood\n",
      "count  2248.000000\n",
      "mean      1.578292\n",
      "std       1.109537\n",
      "min       0.000000\n",
      "25%       1.000000\n",
      "50%       2.000000\n",
      "75%       3.000000\n",
      "max       3.000000\n",
      "           artist               title  mood original_mood  \\\n",
      "0  George Michael     I Want Your Sex     1         happy   \n",
      "1      Rob Zombie        Pussy Liquor     0         angry   \n",
      "2     Bing Crosby  Swinging On A Star     1         happy   \n",
      "3        Ludacris            Get Back     0         angry   \n",
      "4            Hole              Violet     0         angry   \n",
      "\n",
      "                                              lyrics  \n",
      "0  There's things that you guess\\nAnd things that...  \n",
      "1  Earl had a baby\\nBaby was her name\\nHe knew sh...  \n",
      "2  Would you like to swing on a star\\nCarry moonb...  \n",
      "3  Heads up (woop, woop)\\nHeads up (woop, woop)\\n...  \n",
      "4  And the sky was made of amethyst\\nAnd all the ...  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 89 rows containing non-English lyrics.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'datasets/cleaned_dataset_with_lyrics.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "display(df.info())\n",
    "display(df.describe())\n",
    "display(df.head())\n",
    "\n",
    "# Data Analysis: Visualize the distribution of the moods\n",
    "sns.countplot(x='mood', data=df)\n",
    "plt.title('Mood Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Data Cleaning: Handle missing values\n",
    "df.dropna(subset=['lyrics', 'mood'], inplace=True)                                     #####SHOULDN'T NEED THIS\n",
    "\n",
    "# Data Cleaning: Feature Selection\n",
    "df = df[['lyrics', 'mood']]\n",
    "\n",
    "# Function to check if the lyrics are in English\n",
    "def eng_ratio(text):\n",
    "    ''' Returns the ratio of English to total words from a text '''\n",
    "    english_vocab = set(w.lower() for w in words.words())\n",
    "    text_vocab = set(w.lower() for w in text.split() if w.lower().isalpha())\n",
    "    unusual = text_vocab.difference(english_vocab)\n",
    "    eng_ratio = (len(text_vocab) - len(unusual)) / len(text_vocab)\n",
    "    return eng_ratio\n",
    "\n",
    "# Filter out non-English lyrics\n",
    "initial_row_count = df.shape[0]\n",
    "df['eng_ratio'] = df['lyrics'].apply(eng_ratio)\n",
    "df = df[df['eng_ratio'] > 0.5]\n",
    "rows_removed_count = initial_row_count - df.shape[0]\n",
    "print(f\"Removed {rows_removed_count} rows containing non-English lyrics.\")\n",
    "\n",
    "# Text Preprocessing function\n",
    "def preprocess_lyrics(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])  # Remove stopwords and lemmatize\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the lyrics\n",
    "df['lyrics'] = df['lyrics'].apply(preprocess_lyrics)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['lyrics'], df['mood'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the processed data\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "# Save the TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "np.save('X_train_tfidf.npy', X_train_tfidf.toarray())\n",
    "np.save('X_test_tfidf.npy', X_test_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.55      0.59       103\n",
      "           1       0.58      0.52      0.55       110\n",
      "           2       0.45      0.34      0.39       107\n",
      "           3       0.40      0.60      0.48       106\n",
      "\n",
      "    accuracy                           0.50       426\n",
      "   macro avg       0.52      0.50      0.50       426\n",
      "weighted avg       0.52      0.50      0.50       426\n",
      "\n",
      "SVM Accuracy: 0.5023474178403756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aanas\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m val_dataset_bert \u001b[38;5;241m=\u001b[39m SongDataset(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyrics\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist(), y_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmood\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist(), bert_tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Train RoBERTa model\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m trainer_roberta \u001b[38;5;241m=\u001b[39m train_transformer_model(RobertaForSequenceClassification, train_dataset_roberta, val_dataset_roberta, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    104\u001b[0m trainer_bert \u001b[38;5;241m=\u001b[39m train_transformer_model(BertForSequenceClassification, train_dataset_bert, val_dataset_bert, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Evaluate models\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 63\u001b[0m, in \u001b[0;36mtrain_transformer_model\u001b[1;34m(model_class, train_dataset, val_dataset, model_name_path)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_transformer_model\u001b[39m(model_class, train_dataset, val_dataset, model_name_path):\n\u001b[0;32m     61\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_path, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(pd\u001b[38;5;241m.\u001b[39mconcat([y_train, y_test])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmood\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()))\n\u001b[1;32m---> 63\u001b[0m     training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     64\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     65\u001b[0m         num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     66\u001b[0m         per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     67\u001b[0m         per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     68\u001b[0m         warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m     69\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m     70\u001b[0m         logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     71\u001b[0m         logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     72\u001b[0m         evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     73\u001b[0m     )\n\u001b[0;32m     75\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     76\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     77\u001b[0m         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     83\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m<string>:128\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1641\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[0;32m   1636\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1640\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1641\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal_than_2_3)\n\u001b[0;32m   1642\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1644\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1645\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1646\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or MLU devices or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1652\u001b[0m     )\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1664\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1665\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:2149\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2146\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2148\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_devices\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:59\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     57\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfget(obj)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:2055\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2055\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2056\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2057\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2058\u001b[0m         )\n\u001b[0;32m   2059\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2060\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "# machine_learning_models.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Load the processed data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_test = pd.read_csv('y_test.csv')\n",
    "X_train_tfidf = np.load('X_train_tfidf.npy')\n",
    "X_test_tfidf = np.load('X_test_tfidf.npy')\n",
    "\n",
    "# Machine Learning Model: SVM\n",
    "svm = SVC(kernel='linear', C=1, random_state=42)\n",
    "svm.fit(X_train_tfidf, y_train.values.ravel())\n",
    "y_pred_svm = svm.predict(X_test_tfidf)\n",
    "\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "\n",
    "# Prepare dataset for RoBERTa and BERT\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, lyrics, labels, tokenizer, max_length):\n",
    "        self.lyrics = lyrics\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lyrics)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lyric = str(self.lyrics[index])\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            lyric,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_transformer_model(model_class, train_dataset, val_dataset, model_name_path):\n",
    "    model = model_class.from_pretrained(model_name_path, num_labels=len(pd.concat([y_train, y_test])['mood'].unique()))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy='epoch'\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': (preds == p.label_ids).astype(np.float32).mean().item()\n",
    "    }\n",
    "\n",
    "# Tokenizer and datasets for RoBERTa\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_dataset_roberta = SongDataset(X_train['lyrics'].values.tolist(), y_train['mood'].values.tolist(), roberta_tokenizer, max_length=128)\n",
    "val_dataset_roberta = SongDataset(X_test['lyrics'].values.tolist(), y_test['mood'].values.tolist(), roberta_tokenizer, max_length=128)\n",
    "\n",
    "# Tokenizer and datasets for BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset_bert = SongDataset(X_train['lyrics'].values.tolist(), y_train['mood'].values.tolist(), bert_tokenizer, max_length=128)\n",
    "val_dataset_bert = SongDataset(X_test['lyrics'].values.tolist(), y_test['mood'].values.tolist(), bert_tokenizer, max_length=128)\n",
    "\n",
    "# Train RoBERTa model\n",
    "trainer_roberta = train_transformer_model(RobertaForSequenceClassification, train_dataset_roberta, val_dataset_roberta, 'roberta-base')\n",
    "trainer_bert = train_transformer_model(BertForSequenceClassification, train_dataset_bert, val_dataset_bert, 'bert-base-uncased')\n",
    "\n",
    "# Evaluate models\n",
    "eval_results_roberta = trainer_roberta.evaluate()\n",
    "eval_results_bert = trainer_bert.evaluate()\n",
    "\n",
    "print(\"RoBERTa Evaluation Results:\")\n",
    "print(eval_results_roberta)\n",
    "\n",
    "print(\"BERT Evaluation Results:\")\n",
    "print(eval_results_bert)\n",
    "\n",
    "# Compare results\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm)}\")\n",
    "print(f\"RoBERTa Accuracy: {eval_results_roberta['eval_accuracy']}\")\n",
    "print(f\"BERT Accuracy: {eval_results_bert['eval_accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
